import streamlit as st
import google.generativeai as genai
import json
import os
import re
import requests
import time

# --- 1. CONFIGURACI√ìN DE P√ÅGINA (ESTILO PULP/RPG) ---
st.set_page_config(
    page_title="Pulp Cthulhu Storyteller", 
    page_icon="üêô", 
    layout="wide" 
)

st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Special+Elite&display=swap');
    
    .stChatMessage {
        max-width: 80% !important;
        margin: auto !important;
    }
    .stMarkdown {
        font-size: 1.25rem !important;
        line-height: 1.8 !important;
        font-family: 'Georgia', serif;
    }
    h1 {
        font-family: 'Special Elite', cursive;
        text-align: center;
        color: #2e4d36;
    }
    .stImage img {
        border: 5px solid #2e4d36;
        border-radius: 10px;
        box-shadow: 5px 5px 15px rgba(0,0,0,0.7);
    }
    </style>
    """, unsafe_allow_html=True)

st.title("üêô Pulp Cthulhu: Cr√≥nicas de 1934")

# --- 2. CONFIGURACI√ìN DE APIS ---
GOOGLE_KEY = st.secrets.get("GOOGLE_API_KEY")
HF_TOKEN = st.secrets.get("HF_TOKEN")

if not GOOGLE_KEY or not HF_TOKEN:
    st.error("Faltan las claves API (GOOGLE_API_KEY o HF_TOKEN) en los Secrets.")
    st.stop()

genai.configure(api_key=GOOGLE_KEY)

# Configuraci√≥n del modelo Gemini 3
generation_config = {
    "temperature": 1.0,
    "top_p": 0.95,
    "response_mime_type": "application/json",
}

# Desactivar censura de Google para el texto narrativo
safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

model = genai.GenerativeModel(
    model_name='gemini-3-flash-preview',
    generation_config=generation_config,
    safety_settings=safety_settings
)

# --- 3. FUNCIONES DE APOYO ---

def generate_flux_image(visual_prompt):
    """Genera imagen con reintentos autom√°ticos si el modelo est√° cargando"""
    API_URL = "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell"
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    
    pulp_prompt = f"Cinematic 1930s pulp adventure style, gritty atmosphere, {visual_prompt}, highly detailed, oil painting texture, dramatic lighting"
    payload = {"inputs": pulp_prompt, "parameters": {"width": 1024, "height": 768}}
    
    max_retries = 5  # Intentaremos hasta 5 veces
    wait_time = 8    # Esperaremos 8 segundos entre intentos
    
    for i in range(max_retries):
        try:
            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)
            
            # Caso 1: Todo OK
            if response.status_code == 200:
                return response.content
            
            # Caso 2: El modelo se est√° cargando (Error 503)
            elif response.status_code == 503:
                st.toast(f"‚è≥ El generador de im√°genes est√° despertando... (Intento {i+1}/{max_retries})")
                time.sleep(wait_time)
                continue # Reintentar
            
            # Caso 3: Error de cuota o l√≠mite (Error 429)
            elif response.status_code == 429:
                st.toast("‚ö†Ô∏è L√≠mite de Hugging Face alcanzado. Esperando un poco m√°s...")
                time.sleep(wait_time * 2)
                continue
                
            else:
                return None
        except Exception as e:
            print(f"Error de conexi√≥n: {e}")
            time.sleep(2)
            
    return None

def clean_json_response(text):
    """Extrae el JSON del texto por si el modelo a√±ade explicaciones"""
    try:
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            return json.loads(match.group(0))
        return None
    except:
        return None

# --- 4. GESTI√ìN DE LA SESI√ìN ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Saludo inicial
    st.session_state.messages.append({
        "role": "model", 
        "parts": ["¬°Bienvenido, aventurero! El motor de tu hidroavi√≥n tose humo negro sobre la selva. El destino te espera. ¬øQui√©n eres y qu√© haces primero?"]
    })

# --- 5. RENDERIZAR CHAT ---
for msg in st.session_state.messages:
    if not msg.get("parts"): continue
    
    with st.chat_message(msg["role"]):
        raw_content = msg["parts"][0]
        data = clean_json_response(raw_content)
        
        if data:
            st.markdown(data.get("historia", ""))
            # Si el mensaje guardado inclu√≠a una imagen (bytes o url), aqu√≠ podr√≠as mostrarla.
            # Para simplificar, en el renderizado hist√≥rico solo mostramos el texto.
            if "visual_data" in msg:
                st.image(msg["visual_data"])
        else:
            st.markdown(raw_content)

# --- 6. BUCLE DE INTERACCI√ìN ---
if prompt := st.chat_input("Escribe tu acci√≥n aqu√≠..."):
    # Mostrar mensaje usuario
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "parts": [prompt]})

    with st.chat_message("assistant"):
        with st.spinner("El Guardi√°n est√° tejiendo la realidad..."):
            
            # Instrucci√≥n de sistema
            system_instruction = """
            Eres un narrador de rol experto en Pulp Cthulhu. 
            Responde SIEMPRE en JSON con esta estructura exacta:
            {
                "historia": "Escribe aqu√≠ la narraci√≥n literaria usando Markdown.",
                "imagen_prompt": "Descripci√≥n visual detallada en INGL√âS para la imagen."
            }
            """
            
            # Construir historial limpio (solo texto)
            history_payload = []
            for m in st.session_state.messages:
                role = "user" if m["role"] == "user" else "model"
                txt = m["parts"][0]
                extracted = clean_json_response(txt)
                if extracted: txt = extracted.get("historia", txt)
                history_payload.append({"role": role, "parts": [txt]})

            try:
                # 1. Obtener texto de Gemini
                chat = model.start_chat(history=history_payload[:-1])
                response = chat.send_message(f"{system_instruction}\n\nAcci√≥n del jugador: {prompt}")
                
                # Extraer texto de la respuesta (manejo robusto de Gemini 3)
                if response.candidates and response.candidates[0].content.parts:
                    raw_text = response.candidates[0].content.parts[0].text
                else:
                    st.error("El modelo no pudo responder. Intenta otra acci√≥n.")
                    st.stop()

                data = clean_json_response(raw_text)
                
                if data:
                    # 2. Mostrar Narraci√≥n
                    st.markdown(data["historia"])
                    
                    # 3. Generar Imagen con FLUX.1
                    img_data = generate_flux_image(data["imagen_prompt"])
                    
                    if img_data:
                        st.image(img_data, caption="Escena visualizada por FLUX.1")
                        # Guardamos los bytes de la imagen en el mensaje para que persista en la sesi√≥n
                        st.session_state.messages.append({
                            "role": "model", 
                            "parts": [raw_text],
                            "visual_data": img_data
                        })
                    else:
                        st.info("‚åõ El generador de im√°genes est√° ocupado, pero la historia contin√∫a...")
                        st.session_state.messages.append({"role": "model", "parts": [raw_text]})
                else:
                    # Fallback si no hay JSON
                    st.markdown(raw_text)
                    st.session_state.messages.append({"role": "model", "parts": [raw_text]})

            except Exception as e:
                st.error(f"Error en el velo de la realidad: {e}")

